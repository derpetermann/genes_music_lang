---
title: "Distance between language polygons"
author: "Peter Ranacher"
date: "10 Juli 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages}
library(rgdal)
library(sp)
library(spdep)
library(raster)
library(adespatial)
library(fields)
```


## A) Distance between random locations per polygon 


```{r random distances}

# Connect to DB 
dsn <- "PG:host='limits.geo.uzh.ch' dbname='limits-db' port=5432 user='contact_zones' password='letsfindthemcontactzones'"

# Fetch random points 
random_points <- readOGR(dsn=dsn, "genetic_ling.random_sample_points_languages")

# Get maximum sample id 
n_sample <- max(random_points$sample_id)

# Small epsilon (for numerical reasons)
epsilon <- 0.1
geo_dbmem <- list()

for (j in 1:n_sample) {
  
    # Get all points of sample j 
    points <- random_points[random_points$sample_id==j, ]

    # Compute distances between all points in the sample 
    mat <- spDists(points, points)
    #rownames(mat) <- points$nam_label
    #colnames(mat) <- points$nam_label
    
    # Compute the mst, find its longest edge and use as threshold 
    mst_1 <- spantree(mat)
    mst_le <- max(mst_1$dist)
    
    # Use the second longest edge 
    #mst_le <- mst_1$dist[order(mst_1$dist, decreasing=TRUE)][2]
    thresh <- mst_le + epsilon 
    
    # Find all nearest neighbors within the distance threshold 
    nb <- dnearneigh(points, 0, thresh)
    
    
    # Normalize the data 
    spwt <- lapply(nbdists(nb, points), function(x) 1 - (x/(4 * thresh))^2)
    
    # Compute weighted neighbor list 
    lw <- nb2listw(nb, style = "B", glist = spwt, zero.policy = TRUE)
    
    # Compute only MEMs which correspond to positive autocorrelation
    res <- scores.listw(lw, MEM.autocor = "positive")
    
    # In case there are no positive MEMs, compute all MEMs and retain first two
    if (length(res) < 2) {
    
      geo_dbmem[[j]] <- NULL
      # Compute all MEMs and retain first two 
      #res <- scores.listw(lw, MEM.autocor = "all")
      #res <- res[, c('MEM1', 'MEM2', 'MEM3')]
      }
    
    else {
      rownames(res) <- points$nam_label
      colnames(res) <- paste("geo_pco_", seq(1,ncol(res)), sep="")
      geo_dbmem[[j]] <- as.data.frame(res)}
    
    if (j%%1000 == 0) {
    print(paste(j, " samples processed"))}
}


sum(sapply(geo_dbmem[1000:5000], is.null, simplify = T))
sum(sapply(geo_dbmem[5000:10000], is.null, simplify = T))

# Distances 

# Read the csv file
data_folder <- "data"
random_distances <- read.csv(file.path(data_folder, "geo/random_distances.csv"), sep=",",
                              header=TRUE)

# Get maximum sample_id
n_sample <- max(random_distances$sample_id)

# Retrieve the names of all distinct languages in alphabetical order
lang_names <- as.character(unique(random_distances$name_a))
lang_names <- lang_names[order(lang_names)]

# Get number of distinct languages
n_lang <- length(lang_names)

# Collect factors for RDA in a list
factors = list(genetics = genetics_pco,
               music = music_pco, 
               grammar = grammar_pc, 
               phonology = phonology_pc)

# This function converts the pairwise distances from the csv to an R object of class "dist"
# In: 
# pw_dist: all pairwise distances 
# n: number of rows and columsn in the distance matrix
# names: language names (ordered alphabetically)
# Out: 
# dist (dist object)

pw_to_r_dist <- function(pw_dist, n, names) {
  
  # Re-order alphabetically 
  pw_dist <- pw_dist[order(pw_dist$name_a,
                           pw_dist$name_b), ]
  
  # Initialize empty matrix 
  mat <- matrix(nrow=n,
                ncol=n)
  
  # name rows and columns
  rownames(mat) <- names
  colnames(mat) <- names
  
  # Fill matrix with distances 
  for (i in 1:nrow(pw_dist)){
    a <- pw_dist[i, "name_a"]
    b <- pw_dist[i, "name_b"]
    d <- pw_dist[i, "distance"]
    
    mat[a, b] <- d}
  
  dist <- as.dist(mat, diag = FALSE, upper = FALSE)
  return(dist)}

# This function merges all factors into one data.frame
merge_list_to_df <- function (list) {
  merged_list = Reduce(merge, lapply(list,
                                     function(x) data.frame(x, rn = row.names(x))))
  rownames(merged_list) <- merged_list$rn
  merged_list$rn <- NULL
  return(merged_list)}


# Loop over all samples    
for (j in 1:n_sample) {
  if (j%%5000 == 0) {
    
    print(paste(j, " samples processed"))
    
    # Retrieve corresponding distances
    sub_rd <- random_distances[random_distances$sample_id == j, ]
    
    # Convert distances to dist object
    geo_dist <- pw_to_r_dist(sub_rd, n_lang, lang_names)
    
    # Compute dbmem
    #geo_dbmem <- dbmem(geo_dist, thresh=NULL, 
    #                   MEM.autocor = "positive", silent = TRUE)
    rownames(geo_dbmem) <- rownames(geo)
    geo_mem <- mgMEM(geo_dist, truncation = NULL, transformation = NULL)$vectorsMEM
    rownames(geo_mem) <- rownames(geo)
    
    # Restructure dbmem results
    geo_pco <- as.data.frame(geo_mem)
    colnames(geo_pco) <- paste("geo_pco_", seq(1,ncol(geo_pco)), sep="")
    factors[["geo"]]<- geo_pco[, c(2,3)]
   
    # Compute RDA 
    
    merged_data = merge_list_to_df(factors)
    
    factors_ordered = list(genetics=merged_data[ , grepl("genetics", colnames(merged_data))],
                        music=merged_data[ , grepl("music", colnames(merged_data))],
                        grammar = merged_data[ , grepl("grammar", colnames(merged_data))],
                        phonology = merged_data[ , grepl("phonology", colnames(merged_data))],                       
                        geo = merged_data[ , grepl("geo", colnames(merged_data))])
  }
}
  
  




```


## B) Distances between polygons / centroids 

```{r polygon distances}

# Import distances (csv)



polygon_distances <- read.csv(file.path(data_folder, "geo/polygon_distances.csv"), sep=",",
                              header=TRUE)

polygon_distances <- polygon_distances[order(polygon_distances$name_a,
                                             polygon_distances$name_b), ]

# Convert distances to distance matrix 

poly_dist_mat <- matrix(nrow=nrow(geo_mat), 
                           ncol=ncol(geo_mat))

centroid_dist_mat <- matrix(nrow=nrow(geo_mat), 
                           ncol=ncol(geo_mat))

rownames(poly_dist_mat) <- rownames(geo_mat)
colnames(poly_dist_mat) <- colnames(geo_mat)

rownames(centroid_dist_mat) <- rownames(geo_mat)
colnames(centroid_dist_mat) <- colnames(geo_mat)

for (i in 1:nrow(polygon_distances)){
  a <- polygon_distances[i, "name_a"]
  b <- polygon_distances[i, "name_b"]
  print(a)
  p_dist <- polygon_distances[i, "shortest_poly_distance"]
  c_dist <- polygon_distances[i, "centroid_distance"]
  poly_dist_mat[a, b] <- p_dist
  centroid_dist_mat[a, b] <- c_dist}
```

## Visualizing the language polygons 

```{r read polygons}

dsn <- "PG:dbname='limits-db' host='limits.geo.uzh.ch' port='5432'user='peter' password='KarenMyLove'"
lang_poly <- readOGR(dsn=dsn,"genetic_ling.language_polygons_compl")

```

