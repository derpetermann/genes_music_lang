---
title: "Distance between language polygons"
author: "Peter Ranacher"
date: "10 Juli 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages}
library(rgdal)
library(sp)
library(spdep)
library(raster)
library(adespatial)
library(fields)
```


## A) Distance between random locations per polygon 


```{r random distances}

# Connect to DB 
dsn <- "PG:host='limits.geo.uzh.ch' dbname='limits-db' port=5432 user='contact_zones' password='letsfindthemcontactzones'"

# Fetch random points 
random_points <- readOGR(dsn=dsn, "genetic_ling.random_sample_points_languages")


random_points_to_dbmem <- function (r_points){

#' This function computes a dbmem for each sample in r_points
#' @param ... r_points: the randome points (SpatialPointsDataFrame with rows nam_label and sample_id)
#' @return a list comprising the dbmem for each sample 
  
  n_sample <- max(r_points$sample_id)
  
  # Small epsilon (for numerical reasons)
  epsilon <- 0.1
  geo_pco  <- list()
  
  for (j in 1:n_sample) {
    
      # Get all points of sample j 
      points <- r_points[r_points$sample_id==j, ]
  
      # Compute distances between all points in the sample 
      mat <- spDists(points, points)
      #rownames(mat) <- points$nam_label
      #colnames(mat) <- points$nam_label
      
      # Compute the mst, find its longest edge and use as threshold 
      mst_1 <- spantree(mat)
      mst_le <- max(mst_1$dist)
      
      # Use the second longest edge 
      #mst_le <- mst_1$dist[order(mst_1$dist, decreasing=TRUE)][2]
      thresh <- mst_le + epsilon 
      
      # Find all nearest neighbors within the distance threshold 
      nb <- dnearneigh(points, 0, thresh)
      
      
      # Normalize the data 
      spwt <- lapply(nbdists(nb, points), function(x) 1 - (x/(4 * thresh))^2)
      
      # Compute weighted neighbor list 
      lw <- nb2listw(nb, style = "B", glist = spwt, zero.policy = TRUE)
      
      # Compute only MEMs which correspond to positive autocorrelation
      res <- scores.listw(lw, MEM.autocor = "positive")
      
      # In case there are no positive MEMs, compute all MEMs and retain first two
      if (length(res) < 2) {
      
        geo_pco[[j]] <- NULL
        # Compute all MEMs and retain first two 
        #res <- scores.listw(lw, MEM.autocor = "all")
        #res <- res[, c('MEM1', 'MEM2', 'MEM3')]
        }
      
      else {
        rownames(res) <- points$nam_label
        colnames(res) <- paste("geo_pco_", seq(1,ncol(res)), sep="")
        res <- res[order(rownames(res)), ]
        geo_pco[[paste('sample_', j, sep="")]] <- as.data.frame(res)}
  
      if (j%%1000 == 0) {
      print(paste(j, " samples processed"))}
  }
  return (geo_pco)
}


# How many NULLS? 
#sum(sapply(geo_dbmem[1000:5000], is.null, simplify = T))
#sum(sapply(geo_dbmem[5000:10000], is.null, simplify = T))

# Compute RDA 

# Best model 

rda_results <- test_rda(factors)

factor_names <- c("genetics", "music", "grammar", "phonology")
best_model <- lapply(factor_names, function (y) {
  lapply(factor_names, function (x) {
    best_m <- rda_results %>%
      filter(response == y) %>% 
      filter(explanatory == x) %>% 
      filter(r2_adj == max(r2_adj, na.rm=T))
  return (best_m)})})

names(best_model) <- factor_names
best_model_simp <- do.call(rbind, lapply(factor_names, function (x) {
  do.call(rbind, best_model[[x]])}))

# Fixed model: Genetics vs. grammar

simple_rda <- function (response, explantory, constraint) {
#' This function computes a dbmem for each sample in r_points
#' @param ... r_points: the randome points (SpatialPointsDataFrame with rows nam_label and sample_id)
#' @return a list comprising the dbmem for each sample 
  rda_results <- list()

  for (i in 1:n_sample) {
    sample <- paste("sample_", i, sep="")
    factors$geo <- geo_pco[[sample]]
    if (is.null(factors$geo)) {
      rda_results[[sample]] <- NULL
    }
    
    else {
      rda <- rda(X = factors$genetics[, c(1:4)], Y = factors$grammar[, c(1:4)], Z = factors$geo)
      r2 <- RsquareAdj(rda)$r.squared
      # Compute the adjusted explained variance
      r2_adj <- RsquareAdj(rda)$adj.r.squared
      # Compute the p-value of the significance test
      sig <- anova.cca(rda, step = 10000)$`Pr(>F)`[1]
      rda_results[[sample]] <- list(r2=r2, r2_adj=r2_adj, sig=sig)
    }
     if (i%%100 == 0) {
      print(paste(i, " samples processed"))
     }
  }
 return(rda_results)
}


r2_adj_full

```


## B) Distances between polygons / centroids 

```{r polygon distances}

# Import distances (csv)



polygon_distances <- read.csv(file.path(data_folder, "geo/polygon_distances.csv"), sep=",",
                              header=TRUE)

polygon_distances <- polygon_distances[order(polygon_distances$name_a,
                                             polygon_distances$name_b), ]

# Convert distances to distance matrix 

poly_dist_mat <- matrix(nrow=nrow(geo_mat), 
                           ncol=ncol(geo_mat))

centroid_dist_mat <- matrix(nrow=nrow(geo_mat), 
                           ncol=ncol(geo_mat))

rownames(poly_dist_mat) <- rownames(geo_mat)
colnames(poly_dist_mat) <- colnames(geo_mat)

rownames(centroid_dist_mat) <- rownames(geo_mat)
colnames(centroid_dist_mat) <- colnames(geo_mat)

for (i in 1:nrow(polygon_distances)){
  a <- polygon_distances[i, "name_a"]
  b <- polygon_distances[i, "name_b"]
  print(a)
  p_dist <- polygon_distances[i, "shortest_poly_distance"]
  c_dist <- polygon_distances[i, "centroid_distance"]
  poly_dist_mat[a, b] <- p_dist
  centroid_dist_mat[a, b] <- c_dist}
```

## Visualizing the language polygons 

```{r read polygons}

dsn <- "PG:dbname='limits-db' host='limits.geo.uzh.ch' port='5432'user='peter' password='KarenMyLove'"
lang_poly <- readOGR(dsn=dsn,"genetic_ling.language_polygons_compl")

```

